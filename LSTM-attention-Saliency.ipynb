{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Data\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import itertools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100\n",
      "loaded 200\n",
      "loaded 300\n",
      "loaded 400\n",
      "loaded 500\n",
      "loaded 600\n",
      "loaded 700\n",
      "loaded 800\n",
      "loaded 900\n",
      "loaded 1000\n",
      "loaded 1100\n",
      "loaded 1200\n"
     ]
    }
   ],
   "source": [
    "d = Data(first=600, x_transpose=(0, 3, 1, 2))\n",
    "X_train,y_train = d.get_train()\n",
    "X_cross, y_cross = d.get_dev()\n",
    "X_test,y_test = d.get_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637, 3, 224, 224)\n",
      "(637,)\n",
      "(196, 3, 224, 224)\n",
      "(196,)\n",
      "(147, 3, 224, 224)\n",
      "(147,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_cross.shape)\n",
    "print(y_cross.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    N = X.shape[0]\n",
    "    return X.view(N,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): \n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "NUM_TRAIN = X_train.shape[0]\n",
    "train_dataset = MyCustomDataset(X_train, y_train)\n",
    "loader_train = DataLoader(train_dataset, batch_size=batch_size,sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)), drop_last = True)\n",
    "\n",
    "cross_dataset = MyCustomDataset(X_cross, y_cross)\n",
    "loader_cross = DataLoader(cross_dataset, batch_size=batch_size, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model): \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            scores = model(Variable(x.float().type(dtypeFloat)))\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == Variable(y.long().type(dtypeLong))).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "    print(img.shape)\n",
    "    img = img.transpose(1,2,0)\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.gca().axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 50\n",
    "print_acc_every = 150\n",
    "show_transformations = False\n",
    "\n",
    "def train(m, optimizer, epochs=15):\n",
    "    loss_arr = []\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            m.train()\n",
    "            scores = m(Variable(x.float().type(dtypeFloat)))\n",
    "            loss = F.cross_entropy(scores, Variable(y.long().type(dtypeLong)))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss))\n",
    "                loss_arr += [loss.item()]\n",
    "                if (t % print_acc_every == 0):\n",
    "                    print (\"train acc:\")\n",
    "                    check_accuracy(loader_train, m)\n",
    "                    print (\"cross acc:\")\n",
    "                    m.eval()\n",
    "                    check_accuracy(loader_cross, m)\n",
    "                    \n",
    "                    # print transformations\n",
    "                    if show_transformations:\n",
    "                        x_ = x[5][None]\n",
    "                        stn = next(m.modules())[0]\n",
    "                        plt.subplot(1, 2, 1)\n",
    "                        i = 50\n",
    "                        imshow_noax(x_.data.numpy()[0], normalize=False)\n",
    "                        plt.subplot(1, 2, 2)\n",
    "                        stn_out = stn(Variable(x_.float().type(dtypeFloat))).data.numpy()[0]\n",
    "                        imshow_noax(stn_out, normalize=False)\n",
    "                        plt.show()\n",
    "                    \n",
    "    return loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_channel_1 = 96\n",
    "attn_channel_2 = 128\n",
    "attn_channel_3 = 256\n",
    "n_class=2\n",
    "\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(            \n",
    "            nn.Conv2d(3, attn_channel_1, 7, stride=1, padding=3), # 224 x 224 x 96\n",
    "            nn.MaxPool2d(2, stride=2), # 112 x 112 x 96\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(attn_channel_1, attn_channel_2, 5, stride=1, padding=2), # 112 x 112 x 128\n",
    "            nn.MaxPool2d(2, stride=2), # 56 x 56 x 128\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(attn_channel_2, attn_channel_3, 3, stride=1, padding=1), # 56 x 56 x 256\n",
    "            nn.MaxPool2d(2, stride=2), # 28 x 28 x 256\n",
    "            nn.ReLU(),\n",
    "#             Flatten(),\n",
    "#             nn.Linear(28*28*256, n_class),\n",
    "        )       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 2\n",
    "C, H, W = 256, 28, 28\n",
    "\n",
    "class MapToClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MapToClass, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(            \n",
    "            Flatten(),\n",
    "            nn.Linear(1 * H * W, n_class),\n",
    "        )       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProduceSaliency(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProduceSaliency, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(            \n",
    "            nn.Conv2d(C, 1, 1, stride=1, padding=0)\n",
    "        )       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size=5\n",
    "C, H, W = 256, 28, 28\n",
    "\n",
    "class AttentiveConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentiveConvLSTM, self).__init__()\n",
    "        \n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.Conv_Wa = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.Conv_Ua = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.Conv_Va = nn.Conv2d(C, 1, filter_size, padding=2)\n",
    "        self.SoftMax_a = nn.Softmax(dim=2)\n",
    "        self.ba = Variable(torch.randn(H, device = device, dtype=dtype, requires_grad=True))\n",
    "        \n",
    "        self.Conv_Wi = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.Conv_Ui = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.bi = Variable(torch.randn(H, device = device, dtype=dtype, requires_grad=True))\n",
    "    \n",
    "        self.Conv_Wf = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.Conv_Uf = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.bf = Variable(torch.randn(H, device = device, dtype=dtype, requires_grad=True))\n",
    "\n",
    "        self.Conv_Wo = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.Conv_Uo = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.bo = Variable(torch.randn(H, device = device, dtype=dtype, requires_grad=True))\n",
    "        \n",
    "        self.Conv_Wc = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.Conv_Uc = nn.Conv2d(C, C, filter_size, padding=2)\n",
    "        self.bc = Variable(torch.randn(H, device = device, dtype=dtype, requires_grad=True))\n",
    "        \n",
    "        self.H0 = Variable(torch.randn(C, H, W, device = device, dtype=dtype, requires_grad=True))\n",
    "        self.C0 = Variable(torch.randn(C, H, W, device = device, dtype=dtype, requires_grad=True))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # put the forward pass here (iterating through the channels of X)\n",
    "        # iteratively call AttentitiveConvLSTM_step\n",
    "        # Return X', which is the features modified with the attention map\n",
    "        Hprev = self.H0\n",
    "        Cprev = self.C0\n",
    "        Hprev = Hprev.unsqueeze(0)\n",
    "        Cprev = Cprev.unsqueeze(0)\n",
    "        \n",
    "        #####################################################\n",
    "        ################### FUCK VINEET #####################\n",
    "        #####################################################\n",
    "        \n",
    "        for i in range (10):\n",
    "            \n",
    "            # Attention layer\n",
    "            tanh_input = self.Conv_Wa(X) + self.Conv_Ua(Hprev) + self.ba\n",
    "            tanh_output = self.Tanh(tanh_input)\n",
    "            Z_t = self.Conv_Va(tanh_output)\n",
    "#             print (i)\n",
    "#             print (X.size())\n",
    "#             print (Z_t.size())\n",
    "            Z_t_reshaped = Z_t.view(batch_size, 1, -1)\n",
    "            A_t_initial = self.SoftMax_a(Z_t_reshaped)\n",
    "            A_t = A_t_initial.view(batch_size, 1, H, W)\n",
    "            A_t_expanded = A_t.expand(X.size())\n",
    "            X_t = A_t_expanded * X\n",
    "\n",
    "            # LSTM layer\n",
    "            I_t = self.Sigmoid(self.Conv_Wi(X_t) + self.Conv_Ui(Hprev) + self.bi)\n",
    "            F_t = self.Sigmoid(self.Conv_Wf(X_t) + self.Conv_Uf(Hprev) + self.bf)\n",
    "            O_t = self.Sigmoid(self.Conv_Wo(X_t) + self.Conv_Uo(Hprev) + self.bo)\n",
    "            G_t = self.Tanh(self.Conv_Wc(X_t) + self.Conv_Uc(Hprev) + self.bc)\n",
    "\n",
    "            Cprev = Cprev * F_t + I_t * G_t\n",
    "            Hprev = O_t * self.Tanh(Cprev)\n",
    "        \n",
    "        return X_t\n",
    "        \n",
    "        \n",
    "#         tanh_input = F.conv2d(X, self.Wa, padding=2) + F.conv2d(Hprev, self.Ua, padding=2) + self.ba\n",
    "#         Z_t = F.conv2d(nn.Tanh(tanh_input), self.Va, stride=1, padding=2)\n",
    "#         print(Z_t.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #         tanh_input1 = F.conv2d(X, (C, C, filter_size, filter_size), stride=1, padding=2)\n",
    "#         tanh_input2 = F.conv2d(Hprev, (C, C, filter_size, filter_size), stride=1, padding=2)\n",
    "#         tanh_input = tanh_input1 + tanh_input2 + self.ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2.2e-4\n",
    "num_classes = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    FeatureExtraction(),\n",
    "    AttentiveConvLSTM(), # outputs attention map\n",
    "    ProduceSaliency(),\n",
    "   # MapToClass(),\n",
    ")\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 0.6990\n",
      "train acc:\n",
      "Got 306 / 620 correct (49.35)\n",
      "cross acc:\n",
      "Got 101 / 180 correct (56.11)\n",
      "Iteration 0, loss = 0.6942\n",
      "train acc:\n",
      "Got 314 / 620 correct (50.65)\n",
      "cross acc:\n",
      "Got 91 / 180 correct (50.56)\n",
      "Iteration 0, loss = 0.6884\n",
      "train acc:\n",
      "Got 315 / 620 correct (50.81)\n",
      "cross acc:\n",
      "Got 79 / 180 correct (43.89)\n",
      "Iteration 0, loss = 0.6911\n",
      "train acc:\n",
      "Got 344 / 620 correct (55.48)\n",
      "cross acc:\n",
      "Got 88 / 180 correct (48.89)\n",
      "Iteration 0, loss = 0.6809\n",
      "train acc:\n",
      "Got 372 / 620 correct (60.00)\n",
      "cross acc:\n",
      "Got 106 / 180 correct (58.89)\n",
      "Iteration 0, loss = 0.6699\n",
      "train acc:\n",
      "Got 385 / 620 correct (62.10)\n",
      "cross acc:\n",
      "Got 108 / 180 correct (60.00)\n",
      "Iteration 0, loss = 0.6848\n",
      "train acc:\n",
      "Got 377 / 620 correct (60.81)\n",
      "cross acc:\n",
      "Got 96 / 180 correct (53.33)\n",
      "Iteration 0, loss = 0.6421\n",
      "train acc:\n",
      "Got 401 / 620 correct (64.68)\n",
      "cross acc:\n",
      "Got 105 / 180 correct (58.33)\n",
      "Iteration 0, loss = 0.7308\n",
      "train acc:\n",
      "Got 338 / 620 correct (54.52)\n",
      "cross acc:\n",
      "Got 84 / 180 correct (46.67)\n",
      "Iteration 0, loss = 0.6543\n",
      "train acc:\n",
      "Got 366 / 620 correct (59.03)\n",
      "cross acc:\n",
      "Got 97 / 180 correct (53.89)\n",
      "Iteration 0, loss = 0.6554\n",
      "train acc:\n",
      "Got 381 / 620 correct (61.45)\n",
      "cross acc:\n",
      "Got 105 / 180 correct (58.33)\n",
      "Iteration 0, loss = 0.6237\n",
      "train acc:\n",
      "Got 418 / 620 correct (67.42)\n",
      "cross acc:\n",
      "Got 113 / 180 correct (62.78)\n",
      "Iteration 0, loss = 0.6342\n",
      "train acc:\n",
      "Got 381 / 620 correct (61.45)\n",
      "cross acc:\n",
      "Got 98 / 180 correct (54.44)\n",
      "Iteration 0, loss = 0.5919\n",
      "train acc:\n",
      "Got 420 / 620 correct (67.74)\n",
      "cross acc:\n",
      "Got 108 / 180 correct (60.00)\n",
      "Iteration 0, loss = 0.6679\n",
      "train acc:\n",
      "Got 430 / 620 correct (69.35)\n",
      "cross acc:\n",
      "Got 111 / 180 correct (61.67)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_arr = train(model, optimizer, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e3d18c780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for t, (x, y) in enumerate(loader_train):\n",
    "    img = model(Variable(x.float().type(dtypeFloat)))\n",
    "    img = img[8]\n",
    "    img = img.data.cpu().numpy()\n",
    "    img = img.reshape((H,W))\n",
    "    imgplot = plt.imshow(img)\n",
    "    break\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.plot(loss_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
