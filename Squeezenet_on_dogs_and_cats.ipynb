{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Data\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100\n",
      "loaded 200\n",
      "loaded 300\n",
      "loaded 400\n",
      "loaded 500\n",
      "loaded 600\n"
     ]
    }
   ],
   "source": [
    "d = Data(first=300, x_transpose=(0, 3, 1, 2))\n",
    "X_train,y_train = d.get_train()\n",
    "X_cross, y_cross = d.get_dev()\n",
    "X_test,y_test = d.get_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317, 3, 224, 224)\n",
      "(317,)\n",
      "(97, 3, 224, 224)\n",
      "(97,)\n",
      "(74, 3, 224, 224)\n",
      "(74,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_cross.shape)\n",
    "print(y_cross.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "else:\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    N = X.shape[0]\n",
    "    return X.view(N,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): \n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = X_train.shape[0]\n",
    "train_dataset = MyCustomDataset(X_train, y_train)\n",
    "loader_train = DataLoader(train_dataset, batch_size=20,sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cross_dataset = MyCustomDataset(X_cross, y_cross)\n",
    "loader_cross = DataLoader(cross_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model): \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            scores = model(Variable(x.float().type(dtypeFloat)))\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == Variable(y.long().type(dtypeLong))).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "    print(img.shape)\n",
    "    img = img.transpose(1,2,0)\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.gca().axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 50\n",
    "print_acc_every = 150\n",
    "show_transformations = False\n",
    "\n",
    "def train(m, optimizer, epochs=15):\n",
    "    loss_arr = []\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            m.train()\n",
    "            \n",
    "            scores = m(Variable(x.float().type(dtypeFloat)))\n",
    "            loss = F.cross_entropy(scores, Variable(y.long().type(dtypeLong)))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss))\n",
    "                loss_arr += [loss.item()]\n",
    "                if (t % print_acc_every == 0):\n",
    "                    print (\"train acc:\")\n",
    "                    check_accuracy(loader_train, m)\n",
    "                    print (\"cross acc:\")\n",
    "                    m.eval()\n",
    "                    check_accuracy(loader_cross, m)\n",
    "                    \n",
    "                    # print transformations\n",
    "                    if show_transformations:\n",
    "                        x_ = x[5][None]\n",
    "                        stn = next(m.modules())[0]\n",
    "                        plt.subplot(1, 2, 1)\n",
    "                        i = 50\n",
    "                        imshow_noax(x_.data.numpy()[0], normalize=False)\n",
    "                        plt.subplot(1, 2, 2)\n",
    "                        stn_out = stn(Variable(x_.float().type(dtypeFloat))).data.numpy()[0]\n",
    "                        imshow_noax(stn_out, normalize=False)\n",
    "                        plt.show()\n",
    "                    \n",
    "    return loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class=2\n",
    "kernel_size =7 \n",
    "stride =2 \n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "        \n",
    "        # Download and load the pretrained SqueezeNet model. \n",
    "#         squeeze_model = models.squeezenet1_1(pretrained=True)\n",
    "        self.model = nn.Sequential(\n",
    "            models.squeezenet1_1(pretrained=True),\n",
    "            nn.Linear(1000,2),\n",
    "        )\n",
    "        \n",
    "#         # https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4\n",
    "        \n",
    "#         # How many In_channels are there for the conv layer\n",
    "#         in_ftrs = squeeze_model.classifier[1].in_channels\n",
    "        \n",
    "#         # How many Out_channels are there for the conv layer\n",
    "#         out_ftrs = squeeze_model.classifier[1].out_channels\n",
    "        \n",
    "#         # Converting a sequential layer to list of layers \n",
    "#         features = list(squeeze_model.classifier.children())\n",
    "        \n",
    "#         # Changing the conv layer to required dimension\n",
    "#         features[1] = nn.Conv2d(in_ftrs, n_class, kernel_size, stride)\n",
    "        \n",
    "#         # Changing the pooling layer as per the architecture output\n",
    "#         features[3] = nn.AvgPool2d(12, stride=1)\n",
    "        \n",
    "#         # Making a container to list all the layers\n",
    "#         squeeze_model.classifier = nn.Sequential(*features)\n",
    "        \n",
    "#         # Mentioning the number of out_put classes\n",
    "#         squeeze_model.num_classes = n_class        \n",
    "\n",
    "        \n",
    "        \n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_channel_1 = 4\n",
    "attn_channel_2 = 10\n",
    "attn_channel_3 = 10\n",
    "attn_channel_4 = 10\n",
    "attn_channel_5 = 8\n",
    "\n",
    "# https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html\n",
    "class STN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STN, self).__init__()\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, attn_channel_1, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(attn_channel_1, attn_channel_2, 3, stride=1, padding=1),\n",
    "#             nn.Conv2d(attn_channel_2, attn_channel_3, 3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(attn_channel_3, attn_channel_4, 3, stride=1, padding=1),\n",
    "#             nn.Conv2d(attn_channel_4, attn_channel_5, 3, stride=1, padding=1),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.ReLU()\n",
    "        )\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(attn_channel_3 * 32 * 32, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        \n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, attn_channel_3 * 32 * 32)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "    \n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 29.9608\n",
      "train acc:\n",
      "Got 168 / 317 correct (53.00)\n",
      "cross acc:\n",
      "Got 48 / 97 correct (49.48)\n",
      "Iteration 0, loss = 0.8635\n",
      "train acc:\n",
      "Got 149 / 317 correct (47.00)\n",
      "cross acc:\n",
      "Got 49 / 97 correct (50.52)\n",
      "Iteration 0, loss = 0.6918\n",
      "train acc:\n",
      "Got 161 / 317 correct (50.79)\n",
      "cross acc:\n",
      "Got 53 / 97 correct (54.64)\n",
      "Iteration 0, loss = 0.6427\n",
      "train acc:\n",
      "Got 185 / 317 correct (58.36)\n",
      "cross acc:\n",
      "Got 48 / 97 correct (49.48)\n",
      "Iteration 0, loss = 0.6344\n",
      "train acc:\n",
      "Got 180 / 317 correct (56.78)\n",
      "cross acc:\n",
      "Got 56 / 97 correct (57.73)\n",
      "Iteration 0, loss = 0.6396\n",
      "train acc:\n",
      "Got 206 / 317 correct (64.98)\n",
      "cross acc:\n",
      "Got 60 / 97 correct (61.86)\n",
      "Iteration 0, loss = 0.6796\n",
      "train acc:\n",
      "Got 213 / 317 correct (67.19)\n",
      "cross acc:\n",
      "Got 50 / 97 correct (51.55)\n",
      "Iteration 0, loss = 0.5383\n",
      "train acc:\n",
      "Got 219 / 317 correct (69.09)\n",
      "cross acc:\n",
      "Got 54 / 97 correct (55.67)\n",
      "Iteration 0, loss = 0.5093\n",
      "train acc:\n",
      "Got 241 / 317 correct (76.03)\n",
      "cross acc:\n",
      "Got 61 / 97 correct (62.89)\n",
      "Iteration 0, loss = 0.5555\n",
      "train acc:\n",
      "Got 242 / 317 correct (76.34)\n",
      "cross acc:\n",
      "Got 66 / 97 correct (68.04)\n",
      "Iteration 0, loss = 0.5096\n",
      "train acc:\n",
      "Got 245 / 317 correct (77.29)\n",
      "cross acc:\n",
      "Got 64 / 97 correct (65.98)\n",
      "Iteration 0, loss = 0.4078\n",
      "train acc:\n",
      "Got 247 / 317 correct (77.92)\n",
      "cross acc:\n",
      "Got 68 / 97 correct (70.10)\n",
      "Iteration 0, loss = 0.6952\n",
      "train acc:\n",
      "Got 211 / 317 correct (66.56)\n",
      "cross acc:\n",
      "Got 56 / 97 correct (57.73)\n",
      "Iteration 0, loss = 0.5307\n",
      "train acc:\n",
      "Got 265 / 317 correct (83.60)\n",
      "cross acc:\n",
      "Got 67 / 97 correct (69.07)\n",
      "Iteration 0, loss = 0.3636\n",
      "train acc:\n",
      "Got 248 / 317 correct (78.23)\n",
      "cross acc:\n",
      "Got 62 / 97 correct (63.92)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f841e0a11d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "channel_1 = 16\n",
    "channel_2 = 32\n",
    "channel_3 = 50\n",
    "channel_4 = 75\n",
    "channel_5 = 50\n",
    "channel_6 = 50\n",
    "channel_7 = 50\n",
    "learning_rate = 2.2e-4\n",
    "num_classes = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    FeatureExtraction(),\n",
    "    #     STN(),\n",
    "#     nn.Conv2d(3, channel_1, 5, padding=2),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2),\n",
    "#     nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
    "#     nn.Conv2d(channel_2, channel_3, 3, padding=1),\n",
    "#     torch.nn.Dropout2d(p=0.5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2),\n",
    "#     nn.Conv2d(channel_3, channel_4, 3, padding=1),\n",
    "#     nn.Conv2d(channel_4, channel_5, 3, padding=1),\n",
    "#     torch.nn.Dropout2d(p=0.5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2),\n",
    "#     nn.Conv2d(channel_5, channel_6, 3, padding=1),\n",
    "#     nn.Conv2d(channel_6, channel_7, 3, padding=1),\n",
    "#     torch.nn.Dropout2d(p=0.5),\n",
    "#     nn.MaxPool2d(2),\n",
    "#     Flatten(),\n",
    "#     nn.Linear(channel_7 * 8 * 8, num_classes)\n",
    ")\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_arr = train(model, optimizer)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.plot(loss_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
