{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Data\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100\n",
      "loaded 200\n",
      "loaded 300\n",
      "loaded 400\n",
      "loaded 500\n",
      "loaded 600\n",
      "loaded 700\n",
      "loaded 800\n",
      "loaded 900\n",
      "loaded 1000\n",
      "loaded 1100\n",
      "loaded 1200\n"
     ]
    }
   ],
   "source": [
    "d = Data(first=600, x_transpose=(0, 3, 1, 2))\n",
    "X_train,y_train = d.get_train()\n",
    "X_cross, y_cross = d.get_dev()\n",
    "X_test,y_test = d.get_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637, 3, 224, 224)\n",
      "(637,)\n",
      "(196, 3, 224, 224)\n",
      "(196,)\n",
      "(147, 3, 224, 224)\n",
      "(147,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_cross.shape)\n",
    "print(y_cross.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "else:\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    N = X.shape[0]\n",
    "    return X.view(N,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): \n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = X_train.shape[0]\n",
    "train_dataset = MyCustomDataset(X_train, y_train)\n",
    "loader_train = DataLoader(train_dataset, batch_size=20,sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cross_dataset = MyCustomDataset(X_cross, y_cross)\n",
    "loader_cross = DataLoader(cross_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model): \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            scores = model(Variable(x.float().type(dtypeFloat)))\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == Variable(y.long().type(dtypeLong))).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "    print(img.shape)\n",
    "    img = img.transpose(1,2,0)\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.gca().axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 50\n",
    "print_acc_every = 150\n",
    "show_transformations = False\n",
    "\n",
    "def train(m, optimizer, epochs=15):\n",
    "    loss_arr = []\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            m.train()\n",
    "            \n",
    "            scores = m(Variable(x.float().type(dtypeFloat)))\n",
    "            loss = F.cross_entropy(scores, Variable(y.long().type(dtypeLong)))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss))\n",
    "                loss_arr += [loss.item()]\n",
    "                if (t % print_acc_every == 0):\n",
    "                    print (\"train acc:\")\n",
    "                    check_accuracy(loader_train, m)\n",
    "                    print (\"cross acc:\")\n",
    "                    m.eval()\n",
    "                    check_accuracy(loader_cross, m)\n",
    "                    \n",
    "                    # print transformations\n",
    "                    if show_transformations:\n",
    "                        x_ = x[5][None]\n",
    "                        stn = next(m.modules())[0]\n",
    "                        plt.subplot(1, 2, 1)\n",
    "                        i = 50\n",
    "                        imshow_noax(x_.data.numpy()[0], normalize=False)\n",
    "                        plt.subplot(1, 2, 2)\n",
    "                        stn_out = stn(Variable(x_.float().type(dtypeFloat))).data.numpy()[0]\n",
    "                        imshow_noax(stn_out, normalize=False)\n",
    "                        plt.show()\n",
    "                    \n",
    "    return loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class=2\n",
    "kernel_size =1 \n",
    "stride =1\n",
    "class FeatureExtraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "        \n",
    "        # Download and load the pretrained SqueezeNet model. \n",
    "        squeeze_model = models.squeezenet1_1(pretrained=True)\n",
    "\n",
    "        # https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4\n",
    "        \n",
    "        # How many In_channels are there for the conv layer\n",
    "        in_ftrs = squeeze_model.classifier[1].in_channels\n",
    "        \n",
    "        # How many Out_channels are there for the conv layer\n",
    "        out_ftrs = squeeze_model.classifier[1].out_channels\n",
    "        \n",
    "        # Converting a sequential layer to list of layers \n",
    "        features = list(squeeze_model.classifier.children())\n",
    "        \n",
    "        # Changing the conv layer to required dimension\n",
    "        features[1] = nn.Conv2d(in_ftrs, n_class, kernel_size, stride)\n",
    "        \n",
    "        # Changing the pooling layer as per the architecture output\n",
    "        features[3] = nn.AvgPool2d(13, stride=1)\n",
    "        \n",
    "        # Making a container to list all the layers\n",
    "        squeeze_model.classifier = nn.Sequential(*features)\n",
    "        \n",
    "        # Mentioning the number of out_put classes\n",
    "        squeeze_model.num_classes = n_class        \n",
    "\n",
    "        self.model = squeeze_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_channel_1 = 4\n",
    "attn_channel_2 = 10\n",
    "attn_channel_3 = 10\n",
    "attn_channel_4 = 10\n",
    "attn_channel_5 = 8\n",
    "\n",
    "# https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html\n",
    "class STN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STN, self).__init__()\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, attn_channel_1, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(attn_channel_1, attn_channel_2, 3, stride=1, padding=1),\n",
    "#             nn.Conv2d(attn_channel_2, attn_channel_3, 3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(attn_channel_3, attn_channel_4, 3, stride=1, padding=1),\n",
    "#             nn.Conv2d(attn_channel_4, attn_channel_5, 3, stride=1, padding=1),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.ReLU()\n",
    "        )\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(attn_channel_3 * 32 * 32, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        \n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, attn_channel_3 * 32 * 32)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "    \n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 15.3284\n",
      "train acc:\n",
      "Got 333 / 637 correct (52.28)\n",
      "cross acc:\n",
      "Got 90 / 196 correct (45.92)\n",
      "Iteration 0, loss = 0.7296\n",
      "train acc:\n",
      "Got 346 / 637 correct (54.32)\n",
      "cross acc:\n",
      "Got 94 / 196 correct (47.96)\n",
      "Iteration 0, loss = 0.6725\n",
      "train acc:\n",
      "Got 345 / 637 correct (54.16)\n",
      "cross acc:\n",
      "Got 95 / 196 correct (48.47)\n",
      "Iteration 0, loss = 0.7915\n",
      "train acc:\n",
      "Got 354 / 637 correct (55.57)\n",
      "cross acc:\n",
      "Got 92 / 196 correct (46.94)\n",
      "Iteration 0, loss = 0.3892\n",
      "train acc:\n",
      "Got 374 / 637 correct (58.71)\n",
      "cross acc:\n",
      "Got 102 / 196 correct (52.04)\n",
      "Iteration 0, loss = 0.4234\n",
      "train acc:\n",
      "Got 359 / 637 correct (56.36)\n",
      "cross acc:\n",
      "Got 97 / 196 correct (49.49)\n",
      "Iteration 0, loss = 0.3254\n",
      "train acc:\n",
      "Got 432 / 637 correct (67.82)\n",
      "cross acc:\n",
      "Got 119 / 196 correct (60.71)\n",
      "Iteration 0, loss = 0.3411\n",
      "train acc:\n",
      "Got 447 / 637 correct (70.17)\n",
      "cross acc:\n",
      "Got 119 / 196 correct (60.71)\n",
      "Iteration 0, loss = 0.2364\n",
      "train acc:\n",
      "Got 547 / 637 correct (85.87)\n",
      "cross acc:\n",
      "Got 144 / 196 correct (73.47)\n",
      "Iteration 0, loss = 0.5068\n",
      "train acc:\n",
      "Got 440 / 637 correct (69.07)\n",
      "cross acc:\n",
      "Got 118 / 196 correct (60.20)\n",
      "Iteration 0, loss = 0.3237\n",
      "train acc:\n",
      "Got 512 / 637 correct (80.38)\n",
      "cross acc:\n",
      "Got 128 / 196 correct (65.31)\n",
      "Iteration 0, loss = 0.1548\n",
      "train acc:\n",
      "Got 449 / 637 correct (70.49)\n",
      "cross acc:\n",
      "Got 126 / 196 correct (64.29)\n",
      "Iteration 0, loss = 0.4852\n",
      "train acc:\n",
      "Got 469 / 637 correct (73.63)\n",
      "cross acc:\n",
      "Got 123 / 196 correct (62.76)\n",
      "Iteration 0, loss = 0.2274\n",
      "train acc:\n",
      "Got 436 / 637 correct (68.45)\n",
      "cross acc:\n",
      "Got 119 / 196 correct (60.71)\n",
      "Iteration 0, loss = 0.0773\n",
      "train acc:\n",
      "Got 461 / 637 correct (72.37)\n",
      "cross acc:\n",
      "Got 123 / 196 correct (62.76)\n",
      "Iteration 0, loss = 0.1402\n",
      "train acc:\n",
      "Got 576 / 637 correct (90.42)\n",
      "cross acc:\n",
      "Got 143 / 196 correct (72.96)\n",
      "Iteration 0, loss = 0.0825\n",
      "train acc:\n",
      "Got 565 / 637 correct (88.70)\n",
      "cross acc:\n",
      "Got 167 / 196 correct (85.20)\n",
      "Iteration 0, loss = 0.1068\n",
      "train acc:\n",
      "Got 606 / 637 correct (95.13)\n",
      "cross acc:\n",
      "Got 151 / 196 correct (77.04)\n",
      "Iteration 0, loss = 0.0806\n",
      "train acc:\n",
      "Got 623 / 637 correct (97.80)\n",
      "cross acc:\n",
      "Got 165 / 196 correct (84.18)\n",
      "Iteration 0, loss = 0.2530\n",
      "train acc:\n",
      "Got 591 / 637 correct (92.78)\n",
      "cross acc:\n",
      "Got 148 / 196 correct (75.51)\n",
      "Iteration 0, loss = 0.0987\n",
      "train acc:\n",
      "Got 630 / 637 correct (98.90)\n",
      "cross acc:\n",
      "Got 171 / 196 correct (87.24)\n",
      "Iteration 0, loss = 0.0018\n",
      "train acc:\n",
      "Got 619 / 637 correct (97.17)\n",
      "cross acc:\n",
      "Got 147 / 196 correct (75.00)\n",
      "Iteration 0, loss = 0.0584\n",
      "train acc:\n",
      "Got 627 / 637 correct (98.43)\n",
      "cross acc:\n",
      "Got 163 / 196 correct (83.16)\n",
      "Iteration 0, loss = 0.0126\n",
      "train acc:\n",
      "Got 631 / 637 correct (99.06)\n",
      "cross acc:\n",
      "Got 158 / 196 correct (80.61)\n",
      "Iteration 0, loss = 0.0083\n",
      "train acc:\n",
      "Got 623 / 637 correct (97.80)\n",
      "cross acc:\n",
      "Got 151 / 196 correct (77.04)\n",
      "Iteration 0, loss = 0.0008\n",
      "train acc:\n",
      "Got 605 / 637 correct (94.98)\n",
      "cross acc:\n",
      "Got 169 / 196 correct (86.22)\n",
      "Iteration 0, loss = 0.1899\n",
      "train acc:\n",
      "Got 632 / 637 correct (99.22)\n",
      "cross acc:\n",
      "Got 172 / 196 correct (87.76)\n",
      "Iteration 0, loss = 0.0897\n",
      "train acc:\n",
      "Got 635 / 637 correct (99.69)\n",
      "cross acc:\n",
      "Got 176 / 196 correct (89.80)\n",
      "Iteration 0, loss = 0.0354\n",
      "train acc:\n",
      "Got 637 / 637 correct (100.00)\n",
      "cross acc:\n",
      "Got 162 / 196 correct (82.65)\n",
      "Iteration 0, loss = 0.0001\n",
      "train acc:\n",
      "Got 636 / 637 correct (99.84)\n",
      "cross acc:\n",
      "Got 176 / 196 correct (89.80)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90c1917cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "channel_1 = 16\n",
    "channel_2 = 32\n",
    "channel_3 = 50\n",
    "channel_4 = 75\n",
    "channel_5 = 50\n",
    "channel_6 = 50\n",
    "channel_7 = 50\n",
    "learning_rate = 2.2e-4\n",
    "num_classes = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    FeatureExtraction(),\n",
    "    #     STN(),\n",
    "#     nn.Conv2d(3, channel_1, 5, padding=2),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2),\n",
    "#     nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
    "#     nn.Conv2d(channel_2, channel_3, 3, padding=1),\n",
    "#     torch.nn.Dropout2d(p=0.5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2),\n",
    "#     nn.Conv2d(channel_3, channel_4, 3, padding=1),\n",
    "#     nn.Conv2d(channel_4, channel_5, 3, padding=1),\n",
    "#     torch.nn.Dropout2d(p=0.5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2),\n",
    "#     nn.Conv2d(channel_5, channel_6, 3, padding=1),\n",
    "#     nn.Conv2d(channel_6, channel_7, 3, padding=1),\n",
    "#     torch.nn.Dropout2d(p=0.5),\n",
    "#     nn.MaxPool2d(2),\n",
    "#     Flatten(),\n",
    "#     nn.Linear(channel_7 * 8 * 8, num_classes)\n",
    ")\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_arr = train(model, optimizer, 30)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.plot(loss_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
